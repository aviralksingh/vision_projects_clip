{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interacting with CLIP\n",
    "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications.\n",
    "\n",
    "Preparation for Colab\n",
    "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the clip package and its dependencies, and check if PyTorch 1.7.1 or later is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.zooxlabs.com/simple/\n",
      "Requirement already satisfied: ftfy in /home/avsingh/anaconda3/lib/python3.11/site-packages (6.2.0)\n",
      "Requirement already satisfied: regex in /home/avsingh/anaconda3/lib/python3.11/site-packages (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/avsingh/anaconda3/lib/python3.11/site-packages (4.66.4)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from ftfy) (0.2.13)\n",
      "Looking in indexes: https://pypi.zooxlabs.com/simple/\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-s92r76qa\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-s92r76qa\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /home/avsingh/anaconda3/lib/python3.11/site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: packaging in /home/avsingh/anaconda3/lib/python3.11/site-packages (from clip==1.0) (23.2)\n",
      "Requirement already satisfied: regex in /home/avsingh/anaconda3/lib/python3.11/site-packages (from clip==1.0) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/avsingh/anaconda3/lib/python3.11/site-packages (from clip==1.0) (4.66.4)\n",
      "Requirement already satisfied: torch in /home/avsingh/anaconda3/lib/python3.11/site-packages (from clip==1.0) (2.2.0)\n",
      "Requirement already satisfied: torchvision in /home/avsingh/anaconda3/lib/python3.11/site-packages (from clip==1.0) (0.16.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.4.99)\n",
      "Requirement already satisfied: numpy in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.32.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from torchvision->clip==1.0) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/avsingh/anaconda3/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model\n",
    "clip.available_models() will list the names of available CLIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avsingh/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/avsingh/anaconda3/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 102MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing\n",
    "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
    "\n",
    "The second return value from clip.load() contains a torchvision Transform that performs this preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7efff7d92660>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "We use a case-insensitive tokenizer, which can be invoked using clip.tokenize(). By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
